import json
import re
import math
from typing import List, Dict

# ==========================================
# 1. æ¨¡æ‹ŸçŸ¥è¯†åº“ (Knowledge Base)
# çœŸå®åœºæ™¯ä¸­ï¼Œè¿™äº›æ˜¯ä»ä¿å•PDFä¸­OCRè¯†åˆ«å¹¶åˆ‡ç‰‡çš„æ•°æ®
# ==========================================
KNOWLEDGE_BASE = [
    {
        "id": "doc_001",
        "section": "å…¬ä¼—è´£ä»»é™©-ç¬¬ä¸‰æ¡",
        "content": "æœ¬ä¿é™©åˆåŒæ‰€æŒ‡çš„ç¬¬ä¸‰è€…ï¼Œæ˜¯æŒ‡é™¤è¢«ä¿é™©äººåŠå…¶é›‡å‘˜ä»¥å¤–çš„ä»»ä½•äººã€‚"
    },
    {
        "id": "doc_002",
        "section": "å…¬ä¼—è´£ä»»é™©-è´£ä»»å…é™¤",
        "content": "å¯¹äºå› ç”µæ¢¯ã€è‡ªåŠ¨æ‰¶æ¢¯å‘ç”Ÿæ•…éšœé€ æˆçš„ç¬¬ä¸‰è€…äººèº«ä¼¤äº¡ï¼Œå¦‚æœè¢«ä¿é™©äººæœªæŒ‰å›½å®¶è§„å®šè¿›è¡Œå®šæœŸæ£€éªŒï¼Œä¿é™©äººä¸æ‰¿æ‹…èµ”å¿è´£ä»»ã€‚"
    },
    {
        "id": "doc_003",
        "section": "é›‡ä¸»è´£ä»»é™©-èµ”å¿èŒƒå›´",
        "content": "åœ¨å·¥ä½œæ—¶é—´å’Œå·¥ä½œåœºæ‰€å†…ï¼Œå› å·¥ä½œåŸå› å—åˆ°äº‹æ•…ä¼¤å®³çš„ï¼Œå±äºå·¥ä¼¤ä¿é™©è´£ä»»èŒƒç•´ï¼Œä¿é™©äººæŒ‰ç…§çº¦å®šè´Ÿè´£èµ”å¿ã€‚"
    },
    {
        "id": "doc_004",
        "section": "è´¢äº§ä¸€åˆ‡é™©-å…èµ”é¢",
        "content": "æ¯æ¬¡äº‹æ•…ç»å¯¹å…èµ”é¢ä¸ºäººæ°‘å¸2000å…ƒæˆ–æŸå¤±é‡‘é¢çš„10%ï¼Œä¸¤è€…ä»¥é«˜è€…ä¸ºå‡†ã€‚"
    }
]

# ==========================================
# 2. ç®€æ˜“å‘é‡æ£€ç´¢ç®—æ³• (Simulated Vector Search)
# çœŸå®åœºæ™¯ä¼šä½¿ç”¨ OpenAI Embedding æˆ– BERT æ¨¡å‹ + Milvus/Faiss
# ==========================================
def get_keywords(text: str) -> set:
    """ç®€å•çš„åˆ†è¯ä¸å…³é”®è¯æå–"""
    # ç®€å•çš„æŒ‰å­—/è¯åˆ‡åˆ†æ¨¡æ‹Ÿ
    return set(re.findall(r"[\u4e00-\u9fa5]{2,}", text))

def jaccard_similarity(query_keywords: set, doc_keywords: set) -> float:
    """è®¡ç®—æ°å¡å¾·ç›¸ä¼¼åº¦"""
    if not query_keywords or not doc_keywords:
        return 0.0
    intersection = len(query_keywords & doc_keywords)
    union = len(query_keywords | doc_keywords)
    return intersection / union

def retrieve_relevant_docs(query: str, top_k: int = 2) -> List[Dict]:
    """æ£€ç´¢æœ€ç›¸å…³çš„æ–‡æ¡£åˆ‡ç‰‡"""
    query_kws = get_keywords(query)
    print(f"\nğŸ” [æ£€ç´¢é˜¶æ®µ] ç”¨æˆ·æé—®å…³é”®è¯: {query_kws}")
    
    scored_docs = []
    for doc in KNOWLEDGE_BASE:
        doc_kws = get_keywords(doc["content"])
        score = jaccard_similarity(query_kws, doc_kws)
        if score > 0:
            scored_docs.append({**doc, "score": score})
    
    # æŒ‰åˆ†æ•°é™åºæ’åˆ—
    scored_docs.sort(key=lambda x: x["score"], reverse=True)
    return scored_docs[:top_k]

# ==========================================
# 3. æ¨¡æ‹Ÿ LLM ç”Ÿæˆ (Simulated LLM Generation)
# çœŸå®åœºæ™¯ä¼šè°ƒç”¨æ–‡å¿ƒä¸€è¨€/GPT-4 API
# ==========================================
def generate_answer(query: str, context_docs: List[Dict]) -> str:
    """ç»„è£… Prompt å¹¶æ¨¡æ‹Ÿç”Ÿæˆå›ç­”"""
    
    # æ„é€  Prompt
    context_text = "\n".join([f"- [{d['section']}]: {d['content']}" for d in context_docs])
    
    prompt = f"""
ã€ç³»ç»ŸæŒ‡ä»¤ã€‘
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä¿é™©ç†èµ”é¡¾é—®ã€‚è¯·åŸºäºä»¥ä¸‹å¼•ç”¨çš„ã€å·²çŸ¥ä¿¡æ¯ã€‘å›ç­”ç”¨æˆ·é—®é¢˜ã€‚
å¦‚æœå·²çŸ¥ä¿¡æ¯æ— æ³•å›ç­”ï¼Œè¯·ç›´æ¥è¯´â€œæˆ‘ä¸çŸ¥é“â€ã€‚

ã€å·²çŸ¥ä¿¡æ¯ã€‘
{context_text}

ã€ç”¨æˆ·é—®é¢˜ã€‘
{query}
    """
    
    print("-" * 50)
    print("ğŸ“ [æ„å»º Prompt] å®é™…å‘é€ç»™å¤§æ¨¡å‹çš„å†…å®¹:")
    print(prompt.strip())
    print("-" * 50)

    # æ¨¡æ‹Ÿ LLM çš„å›ç­”é€»è¾‘ (ç¡¬ç¼–ç æ¼”ç¤ºç”¨)
    if not context_docs:
        return "æŠ±æ­‰ï¼ŒçŸ¥è¯†åº“ä¸­æ²¡æœ‰æ‰¾åˆ°ç›¸å…³æ¡æ¬¾ï¼Œæ— æ³•å›ç­”æ‚¨çš„é—®é¢˜ã€‚"
    
    if "ç”µæ¢¯" in query:
        return (
            "ğŸ¤– [AI å›ç­”]: æ ¹æ®ã€Šå…¬ä¼—è´£ä»»é™©-è´£ä»»å…é™¤ã€‹æ¡æ¬¾ï¼Œå¦‚æœç”µæ¢¯æ•…éšœé€ æˆç¬¬ä¸‰è€…ä¼¤äº¡ï¼Œ"
            "ä¸”è¢«ä¿é™©äººæœªæŒ‰è§„å®šè¿›è¡Œå®šæ£€ï¼Œä¿é™©äººæ˜¯ä¸æ‰¿æ‹…èµ”å¿è´£ä»»çš„ã€‚å»ºè®®æ‚¨æä¾›æœ€æ–°çš„ç”µæ¢¯ç»´ä¿è®°å½•å’Œå¹´æ£€åˆæ ¼è¯ã€‚"
        )
    elif "å…èµ”" in query:
        return (
            "ğŸ¤– [AI å›ç­”]: æ ¹æ®ã€Šè´¢äº§ä¸€åˆ‡é™©-å…èµ”é¢ã€‹è§„å®šï¼Œæ¯æ¬¡äº‹æ•…æœ‰ 2000 å…ƒæˆ–æŸå¤±é‡‘é¢ 10% çš„ç»å¯¹å…èµ”é¢ï¼Œ"
            "ç†èµ”æ—¶ä¼šæ‰£é™¤è¿™éƒ¨åˆ†é‡‘é¢ï¼ˆå–ä¸¤è€…è¾ƒé«˜è€…ï¼‰ã€‚"
        )
    else:
        return f"ğŸ¤– [AI å›ç­”]: æ ¹æ®{context_docs[0]['section']}ï¼Œç›¸å…³è§„å®šä¸ºï¼š{context_docs[0]['content']}"

# ==========================================
# 4. ä¸»ç¨‹åºå…¥å£
# ==========================================
def main():
    print("ğŸš€ å¯åŠ¨ RAG æ£€ç´¢å¢å¼ºç”Ÿæˆæ¼”ç¤º...\n")
    
    # æµ‹è¯•æ¡ˆä¾‹ 1
    user_query = "ç”µæ¢¯å‡ºäº‹æ•…äº†ï¼Œä¿é™©å…¬å¸èµ”ä¸èµ”ï¼Ÿ"
    print(f"ğŸ‘¤ ç”¨æˆ·æé—®: {user_query}")
    
    # Step 1: æ£€ç´¢
    relevant_docs = retrieve_relevant_docs(user_query)
    print(f"âœ… [æ£€ç´¢ç»“æœ] æ‰¾åˆ° {len(relevant_docs)} æ¡ç›¸å…³çŸ¥è¯†")
    
    # Step 2: ç”Ÿæˆ
    answer = generate_answer(user_query, relevant_docs)
    print(answer)
    print("\n" + "="*60 + "\n")

    # æµ‹è¯•æ¡ˆä¾‹ 2
    user_query = "è´¢äº§é™©ä¸€èˆ¬è¦æ‰£å¤šå°‘å…èµ”é¢ï¼Ÿ"
    print(f"ğŸ‘¤ ç”¨æˆ·æé—®: {user_query}")
    
    relevant_docs = retrieve_relevant_docs(user_query)
    print(f"âœ… [æ£€ç´¢ç»“æœ] æ‰¾åˆ° {len(relevant_docs)} æ¡ç›¸å…³çŸ¥è¯†")
    answer = generate_answer(user_query, relevant_docs)
    print(answer)

if __name__ == "__main__":
    main()
